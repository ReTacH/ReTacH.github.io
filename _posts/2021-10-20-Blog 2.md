---
layout: post
title: Blog 2
---
In this blog post, we will build a cool web-scraper for the IMDB website, and answer the following question: *What movie or TV shows share actors with your favorite movie or show?*

For completion, here's a link to my project repository in case you want to implement it by yourself: [IMDB_scraper](https://github.com/ReTacH/IMDB_scraper). Now, let's dig in!

## Overview of the Scraper: Scrapy and Beyond 
Here, we briefly go over the general guideline for the web-scraper. 

The Python module we use is the **Scrapy** which will act like a spider and crawl the website for us. To start the project, we initialize our scraper by calling `"scrapy startproject IMDB_scraper"` on the location we want. This function will create couple files in the folder *IMDB_scraper*; however, we don't really need to touch most of them. Here, we will create our own custom spider inside the *spiders* directory called *imdb_spider.py*. Later on, we will implement our scraper by calling `scrapy crawl imdb_spider -o results.csv` 
## Creating a Custom Spider
First, we will create some initializers to our spider with the following line 
```python
import scrapy
class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'
    start_urls = ['https://www.imdb.com/title/tt6723592/']
```
Here, the *start_urls* is supposed to link to your favorite IMDB movie page. In my case, it's for **TENET**, a sci-fi action thriller movie with lots of cool idea about time reversal written by Christopher Nolan. Next, we will implement three parsing methods for the ImdbSpider class to navigate the website.
#### Implementation of parse(self, response)
In this method we assume that our scraper starts on an IMDB movie pages (as indicated by the start_urls we put in the initializer). We then parse this method by navigating the scraper to the Cast and Crew page. We note that in the cast page, we have an addition "fullcredits" line in the url. So well, just join this line to our original url and we should be done! Once there, we will task our scraper to call parse_full_credits next. All in all, the short code for this method is shown below:  
```python
""" 
This parse method meants to start on an IMDB movie pages 
Calling the function navigates to its corresponding Cast&Crew page
This method does not return any data
"""
cast_urls = response.urljoin("fullcredits/")
yield scrapy.Request(cast_urls, callback = self.parse_full_credits)
```
#### Implementation of parse_full_credits(self, response)
Now we are at the Cast and Crew page!. Next, we want to hop on each actor personal IMDB website. Here, using HTML inspection, we see that the relative urls to these actor webpage lies inside the "td.primary_photo" css bracket. We then pull the relative urls out by drawing the attribute "href" out from the "a" component. At last, we join this relative path to our original urls to construct a full urls to each actor website. At last, we create a for loop to yield a task for the scraper to scrape each actor next using the parse_actor_page method:
```python
"""
This parse method meants to parse the Cast&Crew page 
Calling the function navigates to the page of each actors listed
This method does not return any data
"""
rel_urls = [a.attrib["href"] for a in response.css("td.primary_photo a")]
actor_urls = [response.urljoin(a) for a in rel_urls]
for urls in actor_urls:
    yield scrapy.Request(urls, self.parse_actor_page)
```
#### Implementation of parse_actor_page(self, response)
Our next task is to get the actor name and the movie on which he/she worked from this actor page. First, to get the name of the actor, we observe using the html inspection that the name is in a text of the "span.itemprop". Hence, we can just use *response.css* with get() method to obtain the name-- pretty easy! Now, note that there are category for film which we are considering. Here, we choose the "Actor" topic, which lies inside the first element of the "div.filmo-category-section", and finally, we actract the name of the movie using getall() on the "b a::text" environment. We use the yield method to create a dictionary to compile the actor name and movie name which we want. Together, we create a code from our previous heuristics below:
```python
"""
This parse method meants to parse the Actor page
Calling the function acquires the name of the actor and movie on which 
he/she worked. It yields a dictionary with two key-value pairs, 
of the form {"actor" : actor_name, "movie_or_TV_name" : movie_or_TV_name}
"""
actor_name = response.css("span.itemprop::text")[0].get()
movie_or_TV_box = response.css("div.filmo-category-section")[0]
movie_or_TV_name = movie_or_TV_box.css("b a::text").getall()
yield {"actor" : actor_name, "movie_or_TV_name" : movie_or_TV_name}
```
## Web-scraping Results
After we finished creating the custom spider, we call the following line `scrapy crawl imdb_spider -o results.csv` inside the command prompt. Woo!, there are couple of lines popping up, and we finally see our results in the *results.csv* file. We then move our small analysis to Jupyter notebook, by creating a panda dataframe with the movie and its corresponding number of shared actress from our web-scraping results using our usual data preparation methods in PIC16A:
```python
Results = pd.read_csv("results.csv")
Results["movie_or_TV_name"] = Results["movie_or_TV_name"].str.split(",")
count_dict = {}
for movies_of_actress in Results["movie_or_TV_name"]:
    for movie in movies_of_actress:
        if movie in count_dict.keys():
            count_dict[movie] += 1
        else:
            count_dict[movie] = 1
df_movie = pd.DataFrame(count_dict.items(), 
                        columns=['Movie', 'Shared_actress'])
df_movie = df_movie.sort_values(by=['Shared_actress'], ascending =  False)
df_movie = df_movie.reset_index(drop = True)
df_movie.head(15)
```
![]({{ site.baseurl }}/images/HW2.png)

**Yes!**, finally we create a pseudo recommendation systems which tells us about the movies which shared the cast with our beloved **TENET**. Let's see, maybe after I submitted this blog post assignment, I will go watch the series **CSI : Crime Scene Investigation** next then!