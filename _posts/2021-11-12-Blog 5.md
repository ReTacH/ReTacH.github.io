---
layout: post
title: Blog 5
---
In this blog post, we will investigate an image classification tasks using TensorFlow; here, our goal is to teach a machine learning algorithm to separate the picture of cats and picture of dogs apart. In particular, we delve into the methods of convolutional neural network and transfer learning to create our machine learning models.

## Data Exploratory
First, we will import some modules for us to use in our upcoming models; we will understand each of them better as we go along:
```python
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import utils 
from tensorflow.keras import datasets, layers, models
```
Now, we will use a large chunk of datasets that TensorFlow has collected to be used for improvement of our machine learning skills. (Here, I copy&paste directly from Phil's code):

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                shuffle=True,
                                                batch_size=BATCH_SIZE,
                                                image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                shuffle=True,
                                                batch_size=BATCH_SIZE,
                                                image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```
Here, while there are couple data-processing toolkits involved, the main takeaways of this chunk for the next part is that we now have three particular datasets: *train_dataset, validation_dataset*, and *test_dataset*, where each data of the datasets comes in a batch of 32 pictures, each of resolution 160 x 160.

In addition, we need to add the following technical lines to read the data more rapidly:
```python
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```
Now, it's time to see what our data looks like. In doing so, we will create a visualization that consists of two rows, where the first row contains random pictures of cats from the dataset and the second row contains random pictures of dogs from the dataset. First, note that calling `train_dataset.take(1)` will give us a piece of dataset (in batch size 32). Now, we can directly create the functions using routines from PIC16A: 
```python
def cat_dog_visualizations():
  class_names = ["Cat", "Dog"] # label of the class
  plt.figure(figsize=(10, 6))
  cat_count, dog_count, i = 0 , 0, 0    # set counters for cat/dog
  for images, labels in train_dataset.take(1):
    cat = np.where(labels.numpy() == 0) # find index with cat pics
    dog = np.where(labels.numpy() == 1) # find index with dog pics
    for i in range(6):
      ax = plt.subplot(2, 3, i + 1)  
      if i < 3:                     # plot first three cats on the first row
        plt.imshow(images[cat[0][i%3]].numpy().astype("uint8")) 
        plt.title(class_names[labels[cat[0][i%3]].numpy()])
      else:                         # plot first three dogs on the second row
        plt.imshow(images[dog[0][i%3]].numpy().astype("uint8"))
        plt.title(class_names[labels[dog[0][i%3]].numpy()])
      plt.axis("off")
  plt.show()
```
Calling `cat_dog_visualizations()` will now give pictures of cute cats and dogs!

![]({{ site.baseurl }}/images/HW5_1.png)

Next, we wish to consider the accuracy for our baseline models by considering the number of labels for the dogs and the cats. To do this, we first create an iterator for the labels:
```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```
Now, we can iterate over the labels and count how many are cats and how many are dogs as follow:
```python
cat_number = 0
dog_number = 0
for label in labels_iterator:
  if label == 0:
    cat_number += 1
  else:
    dog_number += 1
print("number of training dataset with label 0 is", cat_number)
print("number of training dataset with label 1 is", dog_number)
```
```
number of training dataset with label 0 is 1000
number of training dataset with label 1 is 1000
```
Hence, we see that exactly half of data is cats and the rest is dogs. Thus, if we were to guess randomly whether a picture is that of a cat or dog, we would expect the accuracy to be identically **50%**.
## First Model
Now, it's time to create our *first* machine learning model! In this model, we create a sequential model in TensorFlow based on convolutional neural network. As shown below, our model here contains 3 layers of *Conv2D* with 16, 16, and 24 units; the model contains 2 layers of *Maxpooling2D*, a layer of *Dropout*, a layer of *Flatten*, and 2 layers of *Dense* with 16 and 2 units. All of these layers are trained using the *relu* activation functions.
```python
model1 = models.Sequential([
    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.05),
    layers.Conv2D(16, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(24, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(16, activation='relu'),
    layers.Dense(2) 
])
model1.compile(optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Note that these number of units are entirely trial and errors. 

![]({{ site.baseurl }}/images/HW5_2.png)

After we've run the model, we observe that the accuracy of our model stabilized between **57% and 60%** during training. This is a bit better than the baseline of 50%. However, here, we observe that the training accuracy blows up to nearly 95%; hence, we expected that there's a massive overfitting going on in our model!

## Second Model with Data Augmentation
Next, we will improve(?) our first model by considering the process of data augmentation: adding modified images of the training dataset into the training dataset itself. Here, the two kinds of augmentations we are looking for are flipping and rotations transformations.

We can create each of these data augmentation layers individually as follows:
```Python
flip = layers.RandomFlip('horizontal')
rotation = layers.RandomRotation(0.2)
```
Here, the argument 'horizontal' in the flip layer tells that the flipping will be done horizontally rather than vertically, and the argument 0.2 in the rotation layer tells that the output would be rotated by a random amount in the range $$[-0.2 \cdot 2\pi, 0.2 \cdot 2\pi ]$$. Let's test how these layers done to our model.
```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = flip(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![]({{ site.baseurl }}/images/HW5_3.png)

```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = rotation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![]({{ site.baseurl }}/images/HW5_4.png)

**NICE!**, we see now that our cats got flipped horizontally, and our dogs got rotated as the name of the layers suggested. Now, we are in a good shape to incorporate these augmentations into our model as follow:
```python
model2 = models.Sequential([
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.1),
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.05),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2) 
])
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Here, note that we adjust the number of units to be quite larger than our first models (as we have more dataset due to the modification). However, the structures of the layers are pretty much the same, except an addition of the two layers *RandomFlip* and *RandomRotation*. Let's see how this model does:

![]({{ site.baseurl }}/images/HW5_5.png)

Here, we observe that the validation accuracy of our model stabilized between **62% and 65%** during training. This is a bit better than the first model we created, but not by much. Instead, we observe that the training accuracy now stabilized nearly around 70%, which is significant lower than our first model. Thus, at least, we see that even though this second model doesn't improve the validation accuracy by much, it helps alleviate the overfitting problem to mild condition.
## Third Model with Data Preprocessing
In this model, we will improve our model by including some preprocessor that aims to optimize the model, such as scaling the colors of the picture to value between 0 and 1 for more efficiency. In doing so, we create our preprocessing layers with the following lines:
```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```
Now, we can slot this preprocessor layer into our machine learning to get our new model:
```python
model3 = models.Sequential([
    preprocessor,
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.1),
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.05),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2) 
])
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

![]({{ site.baseurl }}/images/HW5_6.png)

Here, we observe that the validation accuracy of our model stabilized between **71% and 74%** during training. Now, compares to our first model with validation accuracy slightly less than 60%, we see a steady increase in accuracy by roughly 15% now, pretty good progress. Though, the training accuracy now jumps up to 85% again. This difference of around 10% between the training accuracy and validation accuracy tells that we still have some overfitting going on in the model.
## Fourth Model with Transfer Learning
Lastly, we shift gears from our previous models by considering a new construction of model by using the pre-existing base models which people have trained for the image classification processes. To do this, we download the layers from *MobileNetV2* using the following codes:
```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```
Now, we can create our new model by combining the data augmentation in our second model and the preprocessor in our third model with the base model from MobileNetV2:
```python
model4 = models.Sequential([
    preprocessor,
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.1),                
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.1),
    layers.Dense(2)
])
model4.compile(optimizer='adam',
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])
```
To see what the parameters in our model looks like, we can directly call `model4.summary()`

![]({{ site.baseurl }}/images/HW5_7.png)

**OMG!**, we see that this model contains more than 2 million parameters. However, most of the parameters are non-trainable which arises from our base model which we just downloaded; there's only 2562 parameters which we need to train. This is relatively small compares to the bunch of parameters we have. Now, it's time to test our model:

![]({{ site.baseurl }}/images/HW5_8.png)

Here, we observe that the validation accuracy of our model stabilized between **96% and 99%** during training; this is crazily good! This model outperforms the three models we have considered so far (well, since we downloaded the pre-trained model, that should be given lol). Indeed, we note that the training accuracy of our model also stabilized around 97%. Hence, we don't see much difference between the training accuracy and validation accuracy; so we expected that not much overfitting is going on in this final model.

###  Score on the Test Data
Now, it's time for our last enjoyable moment: testing our model on the test dataset. We can do this directly by calling `loss, accuracy = model4.evaluate(test_dataset)`, and it turns out that our results is
<center>loss: 0.1658 - accuracy: 0.9688</center>
Thus, we see that our model does surprisingly well on the test dataset with accuracy of roughly **97%**, *super great!*