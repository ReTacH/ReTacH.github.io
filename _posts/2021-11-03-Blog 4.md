---
layout: post
title: Blog 4
---
In this blog post, we will motivate and create our own *spectral clustering* algorithm, designed to find clusters in our data points. As the name of the method suggested, the main underlying theory lies on the calculation of the eigenvector corresponding to the second-smallest eigenvalue of the graph's Laplacian matrix. 

Here, I will go over the motivations of the algorithm step-by step and give a condensed end-product of the spectral clustering algorithm at the end. Now, let's dig in!

### Part A: Construction of Similarity matrix 
First, we assume that a data **X**, which is in the form of an `n X 2` matrix representing `n` data points in a 2-dim Euclidean space. While the instruction used the term *similarity matrix*, we basically create, in graph-theory terminology, an *adjacency* matrix for a graph, where two nodes share an edge if its corresponding data points lie within distance *epsilon* to each other. 

For this part, we shall set *epsilon = 0.4*, and the adjacency matrix could be constructed easily be finding pairwise distances between data points  

```python
A = (sklearn.metrics.pairwise_distances(X) < epsilon)*1
np.fill_diagonal(A,0)
```
Note that here, we required that the value along the diagonal be zero instead of one.

### Part B: Defining the Cut Objective
Next, our goal is to partition the dataset we have into two clusters $$C_0$$ and $$C_1$$. In doing so, we need a measurement which tells how **good** our partition does. Here, we consider a *binary norm cut objective* defined by 
<center> $$N_A(C_0,C_1) := cut(C_0,C_1)\left(\frac{1}{vol(C_0)}+\frac{1}{vol(C_1)}\right)$$</center>
Here, there're two more functions which we need to understand: $$cut(C_0,C_1)$$ and $$vol(C_0), vol(C_1)$$. We shall go over these two functions one by one.

**First**, $$ cut(C_0,C_1) $$ tells how many **edges** connects between nodes from **different** clusters $$C_0$$ and $$C_1$$. Thus, a good cut signifies that we were able to successfully separate the two clusters apart well. We can define this function by:

```python 
def cut(A,y):
    cutvalue = 0
    C0 = np.where(y == 0)[0]
    C1 = np.where(y == 1)[0]
    for i in C0:
        for j in C1:
            cutvalue += A[i,j]
    return cutvalue
```
We can test this cut function on the true labels and the random labels  `y_random = np.random.randint(2, size = n)`. Here, we see that cut objective for the true labels is **13** while cut objective for the random labels is **1142**. Hence, the value of cut objective is much smaller for the true labels than the random ones.

**Second**, $$vol(C)$$ tells how many **edges** connects between nodes from the **same** cluster $$C$$. Thus, this inadvertently how *large* is our cluster based on number of edges inside. We can define this function directly by:

```python
def vols(A,y):
    C0 = np.where(y == 0)[0]
    C1 = np.where(y == 1)[0]
    v0 = A[C0,:].sum(axis = 1).sum()
    v1 = A[C1,:].sum(axis = 1).sum()
    return v0,v1
```
all in all, we can now define the binary norm cut objective by combining the two functions we have just created:

```python
def normcut(A,y):
    v0,v1 = vols(A,y)
    return cut(A,y)*(1/(v0)+1/(v1))
```
Here, this norm cut function objective aims to find two clusters which have the small number of edges connecting between them and that each of the clusters are not too small. Indeed, we observe that the true labels give the norm cut objective of **0.0115** while the random labels give the norm cut objective of **1.0123**. Hence, our aim to minimizing this function is reasonable. 

### Part C: Transforming the Labeling
Here, it turns out that optimizing the norm cut objective over the cluster vector $$y$$ is quite troublesome. One way to connect this norm cut directly to linear algebra is to define a new vector $$z$$ such that $$z_i = \frac{1}{vol(C_0)} \quad \text{if} \quad y_i = 0$$ and $$z_i = -\frac{1}{vol(C_1)} \quad \text{if} \quad y_i = 1$$. Under this smart transformation, it turns out that we have
<center> $$N_A(C_0,C_1) = \frac{z^{T}(D-A)z}{z^T D z}   $$</center> 
Now, let's right some functions for this transformation and check whether our equation is veritable. For the transformation, we can just returns a linear function depending on y which gives a value of $$\frac{1}{vol(C_0)}$$ if $$y_i = 0$$ and $$-\frac{1}{vol(C_1)}$$ if $$y_i = 1$$:
```python
def transform(A,y):
    v0,v1 = vols(A,y)
    return -(1/(v0)+1/(v1))*y + (1/(v0))
```
Next, we check whether our equation is verified using the `np.isclose()` functions as follow:
```python
D = np.diag(np.sum(A,axis = 1))
z = transform(A,y)
LHS = normcut(A,y)
RHS = (z@(D-A)@z)/(z@D@z)
print(np.isclose(LHS,RHS))
print(np.isclose(z@D@(np.ones(n)), 0))
```
```
True
True
```

Hence, we see here that the equation above that relates the matrix product with the norm cut is actually true! and also, we have $$z^T D 1 = 0$$ as well.

### Part D: Optimizing the Norm-Cut
While there is some troublesome conditions which we need to satisfy: $$z^T D 1 = 0$$ for our minimization of the norm cut. It turns out that we can just approximate it using the orthogonal complement of z relative to D1. Hence, using the *minimize* function from *scipy.optimize* and the codes originally from Phil's, we get the minimizing vector as follows:

```python
import scipy
def orth(u, v):
    return (u @ v) / (v @ v) * v
e = np.ones(n) 
d = D @ e
def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

z_ = scipy.optimize.minimize(orth_obj, np.ones(n)).x
```
Note here that we start our optimization by initializing the vector with identical ones.

### Part E: Plotting the Results of Clustering
Now, we can plot the clustering by considering whether the value of z_ is positive or negative. Here, just call the following lines:
```python
plt.scatter(X[:,0], X[:,1], c = (z_<0)*1)
```
![]({{ site.baseurl }}/images/HW4_E.png)

**NICE!**, it seems we quite get the kind of clustering we want it to be.

### Part F: Alternative Way of Optimizing the Norm-Cut
Note, it turns out that our previous optimization method of norm-cut is quite expensive, where we need to resort to the orthogonal complement instead. Fortunately, there is a theorem called "**Rayleigh-Ritz**" which basically says that the minimizing vector z we want is actually the eigenvector corresponding to the *second*-smallest eigenvector of the graph normalized Laplacian $$L = D^{-1}(D-A) $$ (And this is why the word "Spectral", dictating the eigenvalues of the matrices come in to play). Now, we can calculate this vector easily as Numpy provides a module for the eigen-decomposition of a matrix:
```python
L = np.linalg.inv(D) @ (D-A)
W,V = np.linalg.eig(L) 
z_eig = V[:,np.argsort(W).tolist().index(1)] 
```
Now, let's see how the clustering works with this method of theoretical-induced optimization by calling:
```python
plt.scatter(X[:,0], X[:,1], c = (z_eig <0)*1)
```
![]({{ site.baseurl }}/images/HW4_F.png)

**COOL!**, we see that we also get a correct clustering we want. Note that here, the performance is slightly better than our previous method of optimization (Our previous one contains a random purple point insides the yellow cluster), and this method is **a lot** faster than before!

### Part G: The Spectral Clustering Algorithm
Now, using all the points we have discussed so far, we can finally create our own concise spectral clustering algorithm!
```python
def spectral_clustering(X,epsilon):
    """
    This function implements the spectral clustering algorithm

    The input of function is as follows:
    X: the datapoints in the form of a matrix where each rows represents
       each data point and each column represents its feature
    epsilon: the distance used to create the similarity matrix,
            it should be a floating number between 0 to 1

    The output of the function is a numpy-array with the same number of
    dimension of the datapoints where the elements dictate which 
    cluster each corresponding data points lie in.

    """
    A = sklearn.metrics.pairwise_distances(X) # Pairwise distance matrix
    A = (A < epsilon)*1                       # Similarity matrix
    np.fill_diagonal(A,0)                     # Fill diagonal with 0
    D = np.diag(np.sum(A,axis = 1))           # Diagonal matrix of degree
    L = np.linalg.inv(D) @ (D-A)              # Laplacian matrix
    W,V = np.linalg.eig(L)                    # Eigenvalue/Eigenvector
    z_eig = V[:,np.argsort(W).tolist().index(1)] # 2nd smallest Eig
    return (z_eig <0)*1                       # Return based on signs
```
Using the function we have just create, we can now directly call
```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.4))
```
to get the plot we saw in Part F straightaway, pretty cool!

### Part H: Some Experiments on the Algorithm
Now, we will try to see how noise comes to play in our algorithm. In this part, we will set epsilon = 0.4 so that we can directly vary the magnitude of the noise in our dataset. Here, we consider 3 simulations where the noises are 0.1, 0.15, and 0.2. Previously, we set the noise of 0.05 and the spectral clustering algorithm works out well. 

#### Simulation 1: Noise = 0.1
![]({{ site.baseurl }}/images/HW4_H1.png)

Here, we see that the spectral clustering algorithm stills work OK, with some mislabels around the end of the moon.
#### Simulation 2: Noise = 0.15
![]({{ site.baseurl }}/images/HW4_H2.png)
As the noise increases more, we see that the two moons now start to merge, and the spectral clustering starts to not be able to distinguish the moons anymore. The results now seem more like a clustering done via K-means
#### Simulation 3: Noise = 0.2
![]({{ site.baseurl }}/images/HW4_H3.png)
Lastly, when the noise is large, we can't distinguish between the two moons, and the spectral clustering now works like a k-means; it couldn't find the underlying moons anymore.

### Part I: Experiment on the Bull's Eye
Lastly, we test our algorithm on the bull's eye data set defined by the following code:
```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, 
                            noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
It turns out that when we set epsilon equals to **0.4**, we obtain the circular clustering we wish:
![]({{ site.baseurl }}/images/HW4_I.png)

Here, we note that the value of epsilon we get is purely experimental; when the value of epsilon is too large or too small, we won't get the kind of circular clustering we want.